{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0a482f-21b1-4603-827f-0c29031d4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c15183b-632b-4961-828d-542b08bd0911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GitHub - DataTalksClub/data-engineering-zoomcamp#prerequisites',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - What are the prerequisites for this course?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)\n",
    "\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c121654-ebe8-4bb6-8725-664c95472d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coontent based ids \n",
    "\n",
    "import hashlib\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    # combined = f\"{doc['course']}-{doc['question']}\"\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    # creates an MD5 hash object using the hashlib.md5() function; \n",
    "    # combined.encode() method converts the combined string into bytes, which is required by the md5 function.\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    # generates the hexadecimal representation of the MD5 hash.\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2732a35-54ec-486c-8319-44f81040834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    doc['id'] = generate_document_id(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f088eec5-c070-43e7-9e50-62a9512e68d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GitHub - DataTalksClub/data-engineering-zoomcamp#prerequisites',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - What are the prerequisites for this course?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'id': '1f6520ca'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b3397-0198-46ec-ad2d-fb126a9e1130",
   "metadata": {},
   "source": [
    "If we donâ€™t include text in our hashes we can see that they are not unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d8b8e1-e31d-480f-b731-cf4657a30f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44cbd68-66b9-407d-b7f3-7139796114bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_id_reduced(doc):\n",
    "   \n",
    "    combined = f\"{doc['course']}-{doc['question']}\"\n",
    "\n",
    "    # creates an MD5 hash object using the hashlib.md5() function; \n",
    "    # combined.encode() method converts the combined string into bytes, which is required by the md5 function.\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    # generates the hexadecimal representation of the MD5 hash.\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "\n",
    "for doc in documents:\n",
    "    doc['id'] = generate_document_id_reduced(doc)\n",
    "\n",
    "hashes = defaultdict(list)\n",
    "\n",
    "for doc in documents:\n",
    "    doc_id = doc['id']\n",
    "    hashes[doc_id].append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0235554-6056-4142-81bf-112cc51005a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_id) == len(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e162f059-b5d4-4a92-8f14-7ee89668aed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca3dc12d 2\n",
      "960fb254 2\n",
      "67d2f21c 2\n",
      "297f443c 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k, values in hashes.items():\n",
    "    if len(values) > 1:\n",
    "        print(k, len(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0d039-aea3-4111-8c73-62c32a2322b4",
   "metadata": {},
   "source": [
    "We have collisions due to duplicate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e460f8e1-c11f-423e-a464-fc8eba11a84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ca3dc12d'},\n",
       " {'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ca3dc12d'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes[\"ca3dc12d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ff099-5e00-4e7d-81ee-b8af0b538bca",
   "metadata": {},
   "source": [
    "Save the indexed docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd515a20-81e9-4d4d-86b0-2b9d2131c40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
